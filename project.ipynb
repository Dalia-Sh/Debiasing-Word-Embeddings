{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debiasing Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.decomposition import PCA\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Reading the embeddings file into a dictionary\n",
    "# Every word will contain a 50-element vector\n",
    "embeddings_index = dict()\n",
    "with open('glove.6B.50d.txt',encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data files\n",
    "with open('./data/definitional_pairs.json') as f:\n",
    "    definitional_pairs = json.load(f)\n",
    "    \n",
    "with open('./data/equalize_pairs.json') as f:\n",
    "    equalize_pairs = json.load(f)\n",
    "\n",
    "with open('./data/gender_specific_seed.json') as f:\n",
    "    gender_specific_words = json.load(f)\n",
    "    \n",
    "with open('./data/professions.json') as f:\n",
    "        professions = json.load(f)\n",
    "        profession_words = [p[0] for p in professions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will take the definitional pairs and calculate the gender subspace\n",
    "# embedding is the embedding we read\n",
    "# We assume dimension k = 10\n",
    "def get_gender_subspace(pairs, embedding, num_components=10):\n",
    "    matrix = []\n",
    "    for a, b in pairs:\n",
    "        # if a pair in definitional_pairs is not in the embedding, then skip it\n",
    "        # otherwise:\n",
    "        if (a in embedding and b in embedding):\n",
    "            # get the mean of the 2 vectors\n",
    "            center = (embedding[a] + embedding[b])/2 # D_i = 2\n",
    "            # then get the deviations of the mean from a and from b\n",
    "            # add these to a matrix\n",
    "            matrix.append(embedding[a] - center)\n",
    "            matrix.append(embedding[b] - center)\n",
    "    # eg if we take 18 pairs, we will get 36 entries\n",
    "    # and a 36x50 matrix for the 18 pairs\n",
    "    matrix = np.array(matrix)\n",
    "    pca = PCA(n_components = num_components)\n",
    "    pca.fit(matrix)\n",
    "    # return 10 vectors where each element will be a 50-dimension vector\n",
    "    return pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing each word in the embedding\n",
    "def normalize(embedding):\n",
    "    for word in embedding:\n",
    "        embedding[word] /= np.linalg.norm(embedding[word])\n",
    "\n",
    "# Get projection on subspace of the word u\n",
    "def get_projection_on_subspace(subspace, u):\n",
    "    temp = subspace.dot(u)\n",
    "    # Expanding dimension from (10,) to (10,1)\n",
    "    temp = np.expand_dims(temp, -1)\n",
    "    # Return a 50-dimensional vector\n",
    "    return np.sum(temp * subspace, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debias_new(embedding, gender_specific_words, definitional_pairs, equalize_pairs):\n",
    "    # Taking a copy of the embedding so that the real embedding is not changed\n",
    "    result_embedding = copy.deepcopy(embedding)\n",
    "    normalize(result_embedding)\n",
    "    \n",
    "    # calculate gender subspace\n",
    "    gender_subspace = get_gender_subspace(definitional_pairs, result_embedding)\n",
    "    projections = dict()\n",
    "    \n",
    "    # projections\n",
    "    specific_set = set(gender_specific_words)\n",
    "    for w in embedding:\n",
    "        # getting projection of w onto the subspace (for each word)\n",
    "        projection_gender_subspace = get_projection_on_subspace(gender_subspace, result_embedding[w])\n",
    "        # Adding to projection dictionary\n",
    "        projections[w] = projection_gender_subspace\n",
    "        # first step of equalizing\n",
    "        # If w is not in set of gender specific words,\n",
    "        # we subtract the gender bias (projection_gender_subspace)\n",
    "        if w not in specific_set:           \n",
    "            result_embedding[w] -= projection_gender_subspace\n",
    "    normalize(result_embedding)\n",
    "    \n",
    "    # making all the words in equalize_pairs lowercase if they are in the embedding\n",
    "    candidate_pairs = [(a.lower(), b.lower()) for (a, b) in equalize_pairs if a in result_embedding and b in result_embedding]\n",
    "    \n",
    "    # second step of equalizing\n",
    "    for (a, b) in candidate_pairs:\n",
    "        mean = (result_embedding[a] + result_embedding[b]) / 2\n",
    "        # Get the projection of mean onto the gender subspace\n",
    "        mean_projection = get_projection_on_subspace(gender_subspace, mean)\n",
    "        v = mean - mean_projection\n",
    "        gender_factor_a = (projections[a] - mean_projection) / np.linalg.norm(projections[a] - mean_projection)\n",
    "        result_embedding[a] = v + np.sqrt(1 - np.linalg.norm(v)**2) * gender_factor_a\n",
    "        gender_factor_b = (projections[b] - mean_projection) / np.linalg.norm(projections[b] - mean_projection)\n",
    "        result_embedding[b] = v + np.sqrt(1 - np.linalg.norm(v)**2) * gender_factor_b\n",
    "            \n",
    "    normalize(result_embedding)\n",
    "    \n",
    "    return result_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_embeddings = debias_new(embeddings_index, gender_specific_words, definitional_pairs, equalize_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.09684225, -0.08374283,  0.04795526, -0.09274136,  0.12638162,\n",
       "        0.00570989, -0.17039476,  0.02421683, -0.08864009,  0.15551344,\n",
       "        0.1187626 ,  0.13448036, -0.18283153, -0.00492178,  0.11353486,\n",
       "       -0.08351187, -0.15023158,  0.15232056, -0.17999893,  0.19482744,\n",
       "       -0.27747056,  0.10796852,  0.03724855,  0.02468305, -0.0898587 ,\n",
       "       -0.3872868 ,  0.05060006, -0.01223798, -0.03468931,  0.00287955,\n",
       "        0.35340136, -0.13069072, -0.08865376,  0.00366421,  0.127917  ,\n",
       "        0.14564325,  0.10240126, -0.11916366,  0.17759123, -0.15560205,\n",
       "       -0.05795389,  0.03828827, -0.31401294,  0.10463749,  0.1282506 ,\n",
       "        0.0411264 ,  0.06590843, -0.08248833,  0.12169588, -0.1201804 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_embeddings['boy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.09421148,  0.16113938, -0.26337108, -0.10037056,  0.15166216,\n",
       "        0.32577285,  0.00312133,  0.08069942, -0.03719027, -0.01015408,\n",
       "        0.10643533, -0.24990745, -0.09128951,  0.03590677,  0.14614901,\n",
       "       -0.02865135, -0.12797943,  0.0571444 , -0.02690467,  0.25316066,\n",
       "        0.06700391,  0.24962664,  0.17628127,  0.23619102, -0.076873  ,\n",
       "       -0.2251927 , -0.18858147,  0.09321316,  0.04423882, -0.0159648 ,\n",
       "        0.21088576,  0.01095933, -0.04905902, -0.01658309,  0.13140553,\n",
       "        0.04803127, -0.15670311, -0.0498042 ,  0.06574442, -0.19474249,\n",
       "       -0.05601468, -0.18147893,  0.09634152, -0.20424075,  0.04536428,\n",
       "       -0.08140942,  0.18661134, -0.02878821,  0.13837434, -0.1228935 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_embeddings['girl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
